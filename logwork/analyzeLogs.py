# Read and process a directory of log files generated by HyGen CMS.

import os
import glob

verbose = True  # Switch dev print statements on and off
log_path = '/Users/billmarty/CMSlogs'
# We're going to create a sub_directory for our output files.
sub_dir = 'dayFiles'


def parse_csv_fields(header, data):
    """Our run log files are stored in csv format, with the first line in the file
        a header that defines the fields.  This function creates a dictionary from
        a header line and a data line by matching up fields."""
    info = {}
    header_fields = str.split(header.rstrip(), ',')
    data_fields = str.split(data.rstrip(), ',')

    # Our header may contain extra fields of information.  If so, truncate it.
    if len(header_fields) != len(data_fields):
        header_fields = header_fields[:len(data_fields)]

    # Match up the lists to create a dictionary.  Include only fields with non-empty data
    for index in range(len(data_fields)):
        if data_fields[index]:
            info[header_fields[index]] = data_fields[index]

    return info


def add_table_entry(table, file, first_line, last_line):
    """When analyzing a batch of run log files, I want answers to a couple of questions:
            Are there gaps in the log?
            Does the engine start or stop in this file?
            Any other obvious anomalies?
        This function fills in a table for at-a-glance analysis of a day's log files."""
    gap_tolerance = 10.0  # Seconds - gaps shorter than this are not flagged.

    entry = {'file': file}
    entry['startTime'] = first_line['linuxtime']
    entry['endTime'] = last_line['linuxtime']
    if table:
        gap = float(entry['startTime']) - float(table[-1]['endTime'])
        if gap > gap_tolerance:
            entry['gap'] = str(int(gap))
    entry['starts'] = str(float(last_line['Engine Starts']) - float(first_line['Engine Starts']))

    table.append(entry)


def main():
    """Read and process a directory of log files generated by HyGen CMS."""

    # Go to the log directory and get a list of the 'run' log files.
    os.chdir(log_path)
    if verbose: print('Working dir: ' + str(os.getcwd()))
    run_files = glob.glob("*run*.csv")
    if verbose: print('There are ' + str(len(run_files)) + ' run log files.')
    run_files.sort()

    # Does our output file sub_directory already exist?
    dir_contents = os.listdir(log_path)
    if sub_dir not in dir_contents:
        if verbose: print('mkdir ' + sub_dir)
        try:
            os.mkdir(sub_dir, mode=0o777)
        except:
            print('!!mkdir exception!!')
    dir_contents = None

    # File name template: year-month-day_hour_run#.csv
    # Get a list of the dates.
    date_list = []
    for file in run_files:
        if '_' in file:
            underscore = file.find('_')
            file_date = file[:underscore]
            if file_date not in date_list:
                date_list.append(file_date)
    if verbose:
        print(str(len(date_list)) + ' unique dates')
        print(date_list)
        print('Most recent date is: ' + str(date_list[-1]))

    # Let's start with the run files for the most recent date.
    # In theory, run_files are already sorted by date.
    recent_date_files = []
    while date_list[-1] in run_files[-1]:
        recent_date_files.append(run_files.pop())
    recent_date_files.reverse()
    if verbose:
        print(str(len(recent_date_files)) + ' recent date files')
        print(recent_date_files)

    # Let's concatenate the recent_date_files into a single file.
    # While concatenating, let's build a table that highlights gaps in the log.
    is_first_header = True  # Only copy the header line once.
    line_count = 0
    table = []
    recent_date_file_name = date_list[-1] + '_day_run.csv'
    sub_dir_file_name = './' + sub_dir + '/' + recent_date_file_name
    if verbose:
        print(recent_date_file_name)
        print(sub_dir_file_name)
    with open(sub_dir_file_name, 'w') as outfile:
        for file in recent_date_files:
            with open(file, 'r') as infile:
                lines = infile.readlines()
                header = lines.pop(0)
                if is_first_header:
                    outfile.write(header)
                    line_count += 1
                    is_first_header = False
                # I've seen at least one run log file where the last line is corrupt or invalid.
                #   So, check the last line, and dump if necessary.
                keep = True
                commas = header.count(',') - 5  # Header has some extra fields.
                if lines[-1].count(',') < commas: keep = False
                # Any more tests?
                if not keep:
                    line_invalid = lines.pop()
                for n, line in enumerate(lines):
                    outfile.write(line)
                    line_count += 1
                # Grab the first and last line info I want for my table.
                first_line = parse_csv_fields(header, lines[0])
                last_line = parse_csv_fields(header, lines[-1])
                add_table_entry(table, file, first_line, last_line)
            # We're done with lines here.  Can we encourage garbage collection
            #   to free the memory?
            del lines[:]
    if verbose: print('Line count: ' + str(line_count))
    if verbose: print('Table entries: ' + str(len(table)))

    # TODO write my table out to a csv file.

if __name__ == '__main__':
    main()
